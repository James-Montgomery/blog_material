{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recongition\n",
    "\n",
    "In a previous post I covered how to build a very simple voice recognition model using guassian mixture models and some pretty neat audio preprocessing techniques. I thought that a follow up with facial recognition would be fun!\n",
    "\n",
    "THe best facial recongition models are deep elarning models thag take advantage of hundreds or even thousands of photos of the target's face. These models tend to be convolutional neural networks (CNNs). You can even treat the parameters of the entworks in a bayesian fashion to get nice uncertainty estiamtes for your recognition model. I've given lectures on this subject and it's pretty cool. However, we'll take a much simpler modeling approach in this post. \n",
    "\n",
    "Here we will go back before the days deep learning and discuss one of the ways that facial recongition was approached. Here is the blog overview:\n",
    "\n",
    "* Setting Everything Up\n",
    "* Capturing Video in Python\n",
    "* Haarcascaades\n",
    "* Facial Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Everything Up\n",
    "\n",
    "In many of my posts I gloss over environment set up. THis isn't best practice, but I don't often end up using any libraries with major backward compatability issues. Numpy, for instance, is fairly reliable across versions. Here I thought it prudent to provide a little more information.\n",
    "\n",
    "To get set up for this blog post, pelase create a new conda virtual environment and install the relevant packages using the commands below:\n",
    "\n",
    "~~~Bash\n",
    "pip install opencv-contrib-python==4.0.0.21\n",
    "pip install opencv-python==4.0.0.21\n",
    "pip install Pillow==4.2.1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Video in Python\n",
    "\n",
    "The basics of capturing video in python care faily simple. If you want to port this fucntionality to a web application with a python backend things get complicated fast. This tutorial is not meant to guide you into creating a production grade system for facial recongition, so forgive me if I just code only suitable for prototyping! \n",
    "\n",
    "Let's start by importing the relevant packages. If you run this code on a raspberry pi I would add the line `img = cv2.flip(img, -1)` below the line `ret, img = cap.read()`. This will flip the image returned by the camera (a quirk of raspberry pi that I've run into is inverted image captures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the code below. You should get a video showing the view from the default camera on your device. On my mac that is the front camera. On my raspberry pi it is the pi cam I attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may be prompted to give python permission\n",
    "# to use your camera when runnign this code\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# video is just a set of image captures in\n",
    "# very quick selection\n",
    "# here we capture on \"frame\" per loop in\n",
    "# our while statement\n",
    "while True:\n",
    "    #capture image/frame\n",
    "    ret, img = cap.read()\n",
    "    # grey scale the image\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # show the gray image \n",
    "    # to show the color image change the \n",
    "    # argument gray to img\n",
    "    cv2.imshow('video',gray)\n",
    "    # check if the suer has pressed a key\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    # if the user hits the escape key\n",
    "    # exit the while loop \n",
    "    if k == 27: \n",
    "        break\n",
    "# clean up the image capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like the video below. The framerate isn't great...but this is python and we never set out to create a production grade application. I also apologize for the scruffy look. It's been a freezing cold winter and the beard keeps me warm.\n",
    "\n",
    "![](video_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haarcascaades\n",
    "\n",
    "Ok, before we do facial recongition we need to find faces to recogntize! Right now we just have a series of images (a video stream). Thart video could be of anything! So how do we find any faces in our images? Well, we could always go back to deep learning, but that is overkill for this particular problem. Here we will sue harcascaades. \n",
    "\n",
    "Haar feature cascaade classifiers are an approach to iamge classification originally proposed by Paul Viola and Michael Jones back in 2001 in the paper [\"Rapid Object Detection using a Boosted Cascade of Simple Features\"](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf). They are a kind of precursor to CNNs. \n",
    "\n",
    "Haar features are derived from a very particualr kind of iamge filter. During model training we arrive at the optimal filters to use for our classification case (similar to filter selection of CNNs). Before Haar cascaades there was a time when human experts would hand pick filters!\n",
    "\n",
    "Here are a few examples of filters that might be used in a cascaade.\n",
    "![](haar_features.png)\n",
    "\n",
    "The features of our model are single values taken by subtracting the sum of the pixels under the white rectangle from the sum of the pixels under tha black rectangle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our filters will find interesting artifacts in our images. For instance, in the image below we seem to be finding all of the edges of a woman's body within an image. It has done this by finding out that certain filters allow us to detect the change in darkness when moving from a light background to the darker shades of the woman's jeans.\n",
    "\n",
    "![](haar_eaxmple_1.png)\n",
    "\n",
    "After passing MANY filters over the image, we can arrive at a holistic view of all of the edges of a particular subject in the image. For example, we seem to have isolated the basic scaffold of the face in the image below using harr filters.\n",
    "\n",
    "![](harr_example_2.jpg)\n",
    "\n",
    "Haar filters are not the most sophisticated means of [edge](https://en.wikipedia.org/wiki/Edge_detection)/feature detection, but they get the job done and are very simple. Given enough positive and neagtive sampels to train on, we can find an efficient set of filters to let us do our detection task. One effective way of finding features is using a method like [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so  we've covered harr features...what is a cascaade? Cascaading is a way of effieciently searching a alrge image for an object. First we pass a window across an image. Our model only 'sees' what is within the window. \n",
    "\n",
    "![](sliding_window_example.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we see a new window we apply a coarse set of filters. If this first set of filters find nothing of interest we move on to the next window in the image. If the first set of filters find something interesting we apply a second set of finer filters. This is a \"cascaade\" of filters (hence the name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so let's find some faces using haarcascaades! First, download a pre-trained facial recognition haarcascaade: [haarcascaade repository](https://github.com/opencv/opencv/tree/master/data/haarcascades). I will be using `haarcascade_frontalface_default.xml` in this tutorial. You can [build your own haarcascaade](http://note.sonots.com/SciSoftware/haartraining.html) if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pretrained cascaade\n",
    "faceCascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # detect the edges of the window the \n",
    "    # cascaade detects the face in\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,     \n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,     \n",
    "        minSize=(20, 20)\n",
    "    )\n",
    "    # plot the dimensions of the window the \n",
    "    # cascaade detected\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]  \n",
    "        \n",
    "    cv2.imshow('video',img)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get results like in the gif below.\n",
    "\n",
    "![](video_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Recognition Model\n",
    "\n",
    "There are many ways of doing facial recognition other than using deep learning. For instance, you could run principal components analysis(PCA) on the images and run logisitic regression on a reduced set of the components! Of course, this would no behave well if there are many variations on color, lighting, position, etc. You might try to handle the inherent variations in the image trains et by trainign some kind of kernel classifier, but this will not scale well to large training sets. \n",
    "\n",
    "Today I'll stick with Local Binary Patterns Histograms (LBPH). This method is reasonably robust to noise, somewhat agnostic to the dimensions of the image, and works ok on very small training sets (as low as single digit counts of images). \n",
    "\n",
    "This is nice because we may only have one image of the subject! For instance, I recently tried to create a facial recognition model ofr a cyber security colleague of mine (for an office prank). He has intentionally scoured his image from the internet as he is a huge security buff! I was only able to get a single devent image of him from a talk at a conference he gave years ago. I could synthesize more iamges using GANs and VAEs to suppliment my training set...instead I chsoe to use LBPH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBPH differs from many other methods in that it does not treat the image as a single vector (as the PCA Logistic regression would, for instance). LBPH takes it's roots from texture analysis and tries to compare every pixel to it's neightboring pixles. If the \"center\" pixel has an intensity greater than or equal to it's neighboring pixel then we label it as 1, if not we label it a 0. See the example below for 3x3 matrix of pixels.\n",
    "\n",
    "![](lbp.png)\n",
    "\n",
    "We could write this formally as such:\n",
    "\n",
    "$$\n",
    "LBP(x_c, y_c) = \\sum_{p=0}^{P-1}2^ps(i_p-i_c)\n",
    "$$\n",
    "\n",
    "Where the subscript c denotes the central pixel and i denotes a pixel intensity. x and y are pixel coordinates. p are the neighboring pixels. s is a sign (not sine) function:\n",
    "\n",
    "$$\n",
    "    s(x)= \n",
    "\\begin{cases}\n",
    "    1, & \\text{if } x\\geq 0\\\\\n",
    "    0,              & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This can be made more robust to scaling issues by defining a variable neighborhood of pixels (I won't cover that here). This method is very sensitive to local structure and can help us find detailed sturctures in our image in a sinialr way to the filters in the har cascaade.\n",
    "\n",
    "![](patterns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some spatial features extracted by LNP, we need a way of comparing the patterns in one image to another to make predictions. We will use a method proposed by Ahonen, Hadid, and Pietikainen in [\"Face Recognition with Local Binary Patterns\"](http://www.ee.oulu.fi/mvg/files/pdf/pdf_494.pdf). We divide the image into sections and extract a histogram of each local region. We then concatenate these histograms to use when comparing images for \"similarity\". I'd really recommend reading the paper. It's a very cool technique and super simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's define our traing image set. In the same directory as your python script is saved, create a folder called `/data`. In this folder create a folder for every subject with the id number you want to assign to the subject (starting at 1). For example, I have four subjects and so I have four folders: `1`, `2`, `3`, `4` for the respective subjects: James with no beard, James with beard, Ken, Hugh Laurie. Save your training image files in these folders.\n",
    "\n",
    "The file structure should look soemthing like this:\n",
    "```\n",
    ".\n",
    "├── face_recognition.py\n",
    "└── data\n",
    "    ├── 1\n",
    "    |   └── img1.jpg\n",
    "    ├── 2\n",
    "    |   └── img2.jpg\n",
    "    ├── 3\n",
    "    |   └── img3.jpg\n",
    "    └── 4\n",
    "        └── img4.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the images and label data\n",
    "def getImagesAndLabels(path):\n",
    "    faceSamples=[]\n",
    "    ids = []\n",
    "    folderPaths = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    folderPaths = [folder for folder in folderPaths if '.DS' not in folder]\n",
    "    for folderPath in folderPaths:\n",
    "        id = int(folderPath.split(\"/\")[-1])\n",
    "        imagePaths = [os.path.join(folderPath,f) for f in os.listdir(folderPath)]\n",
    "        imagePaths = [img for img in imagePaths if '.DS' not in img]\n",
    "        for imagePath in imagePaths:\n",
    "            PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n",
    "            img_numpy = np.array(PIL_img,'uint8')\n",
    "            faces = detector.detectMultiScale(img_numpy)\n",
    "            for (x,y,w,h) in faces:\n",
    "                faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
    "                ids.append(id)\n",
    "    return faceSamples, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the training examples I will use. I'll be tough on the model and force it to use only one training sample per class. Hopefully you'll ahve more than just one sample!\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./data/1/6.jpg\" width=\"100\">\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./data/2/2.jpg\" width=\"100\">\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./data/3/3.jpg\" width=\"100\">\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./data/4/4.jpg\" width=\"100\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create our face recognition mode and load in the haarcascade for the face detector. We also load in all of our training images and then train our LBPH model. We save the model in case we want to use it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = cv2.face.LBPHFaceRecognizer_create() \n",
    "detector = cv2.CascadeClassifier(\"./haarcascade_frontalface_default.xml\")\n",
    "\n",
    "path = './data'\n",
    "faces,ids = getImagesAndLabels(path)\n",
    "recognizer.train(faces, np.array(ids))\n",
    "recognizer.write('./trainer.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.read('./trainer.yml')\n",
    "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# names related to ids: example ==> Marcelo: id=1,  etc\n",
    "names = ['None','James No Beard','James Beard', 'Ken', 'Hugh Laurie'] \n",
    "\n",
    "# Initialize and start realtime video capture\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3, 640) # set video widht\n",
    "cam.set(4, 480) # set video height\n",
    "\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1*cam.get(3)\n",
    "minH = 0.1*cam.get(4)\n",
    "\n",
    "while True:\n",
    "    ret, img =cam.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale( \n",
    "        gray,\n",
    "        scaleFactor = 1.2,\n",
    "        minNeighbors = 5,\n",
    "        minSize = (int(minW), int(minH)),\n",
    "       )\n",
    "    \n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "        i, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "        \n",
    "        # Check if confidence is less them 100 ==> \"0\" is perfect match\n",
    "        if (confidence <= 100 and confidence >= 50):\n",
    "            i = \"uncertain\"\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        elif (confidence <= 100):\n",
    "            i = names[i]\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        else:\n",
    "            i = \"unknown\"\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        \n",
    "        cv2.putText(img, str(i), (x+5,y-5), font, 1, (255,255,255), 2)\n",
    "        cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)  \n",
    "    \n",
    "    cv2.imshow('camera',img) \n",
    "    k = cv2.waitKey(10) & 0xff \n",
    "    if k == 27:\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the final product should look like! We display a \"confidence\" number that represents (at a high level) the distance between the historgrams of the training images for the predicted class and the image we are trying to classify.\n",
    "\n",
    "![](video_3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parting Words\n",
    "\n",
    "I hope you enjoyed this post! I promnise I'll shave before doing any more image recongition posts. If you want to take the concepts here even further, here are some suggestions:\n",
    "* Preprocessing Steps\n",
    "    * [Align faces to increase prediction accuracy](https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/)\n",
    "    * Preprocessing light, color balance, etc. to increase prediction accuracy\n",
    "    * Add in a voice recognition model\n",
    "    * Replace Haarcascaade with deep learning model\n",
    "    * Replace LBPH with deep learning facial recognition model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
