{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hogwild!\n",
    "https://srome.github.io/Async-SGD-in-Python-Implementing-Hogwild!/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing.sharedctypes import Array\n",
    "from ctypes import c_double\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10 # number of features\n",
    "m=20000 # number of training examples\n",
    "\n",
    "X = scipy.sparse.random(m,n, density=.2).toarray() # Guarantees sparse grad updates\n",
    "X = X/X.max() # Normalizing for training\n",
    "\n",
    "real_w = np.random.uniform(0,1,size=(n,1)) # Define our true weight vector\n",
    "\n",
    "y = np.dot(X,real_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_shared = Array(c_double, \n",
    "        (np.random.normal(size=(n,1)) * 1./np.sqrt(n)).flat,\n",
    "        lock=False) # Hogwild!\n",
    "w = np.frombuffer(coef_shared)\n",
    "w = w.reshape((n,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The calculation has been adjusted to allow for mini-batches\n",
    "learning_rate = .001\n",
    "def mse_gradient_step(X_y_tuple):\n",
    "    global w # Only for instructive purposes!\n",
    "    X, y = X_y_tuple # Required for how multiprocessing.Pool.map works\n",
    "    \n",
    "    # Calculate the gradient\n",
    "    err = y.reshape((len(y),1))-np.dot(X,w)\n",
    "    grad = -2.*np.dot(np.transpose(X),err)/ X.shape[0]\n",
    "\n",
    "    # Update the nonzero weights one at a time\n",
    "    for index in np.where(abs(grad) > .01)[0]:\n",
    "        coef_shared[index] -= learning_rate*grad[index,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "examples=[None]*int(X.shape[0]/float(batch_size))\n",
    "for k in range(int(X.shape[0]/float(batch_size))):\n",
    "    Xx = X[k*batch_size : (k+1)*batch_size,:].reshape((batch_size,X.shape[1]))\n",
    "    yy = y[k*batch_size : (k+1)*batch_size].reshape((batch_size,1))\n",
    "    examples[k] = (Xx, yy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function on the training set: 0.028062460390419135\n",
      "Difference from the real weight vector: 0.41999701749960905\n"
     ]
    }
   ],
   "source": [
    "# Training with Hogwild!\n",
    "p = Pool(5)  \n",
    "p.map(mse_gradient_step, examples)\n",
    "\n",
    "print('Loss function on the training set:', np.mean(abs(y-np.dot(X,w))))\n",
    "print('Difference from the real weight vector:', abs(real_w-w).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
